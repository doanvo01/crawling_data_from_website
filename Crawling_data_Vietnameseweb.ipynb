{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932089",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-24T04:19:46.612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=1\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=2\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=3\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=4\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=5\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=6\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "MAJOR = \"\"\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"Get html from url\"\"\"\n",
    "    try:\n",
    "        page_content = requests.get(url)\n",
    "\n",
    "        if page_content.status_code == 200:\n",
    "            soup = BeautifulSoup(page_content.text, 'html.parser')\n",
    "\n",
    "            item = {}\n",
    "            #get date\n",
    "            date = soup.find('span', class_='news-time')\n",
    "            if date:\n",
    "                item['published_date'] = date.text.strip()\n",
    "            # Get title\n",
    "            title = soup.find('h1', class_='title')\n",
    "            if title:\n",
    "                item['title'] = title.text.strip()\n",
    "            # Get content\n",
    "            content = soup.find('section', id=\"news-content\")\n",
    "            if content:\n",
    "                #get question\n",
    "                question = content.find('strong', class_=\"sapo\")\n",
    "                if question:\n",
    "                    item['question'] = question.text.strip()\n",
    "                #remove question\n",
    "                question.extract()\n",
    "                #remove div with id= \"accordionMucLuc\" from content\n",
    "                accordion = content.find('div', id=\"accordionMucLuc\")\n",
    "                if accordion:\n",
    "                    accordion.extract()\n",
    "                #get content\n",
    "                # item['content'] = content.text.strip()\n",
    "                parts = content.find_all(['p','h2', 'blockquote'])\n",
    "                # pprint(parts)\n",
    "                news_content = \"\"\n",
    "                prev_part = None\n",
    "                for part in parts[:-1]:\n",
    "                    if part.name == \"p\":\n",
    "                        if prev_part and prev_part.name == \"blockquote\":\n",
    "                            news_content += \"</ref>\\n\" + part.text.strip() + \"\\n\"\n",
    "                        else:\n",
    "                            news_content += part.text.strip() + \"\\n\"\n",
    "                    elif part.name == \"blockquote\":\n",
    "                        if prev_part and prev_part.name != \"blockquote\":\n",
    "                            news_content += \"<ref>\" + part.text.strip() + \"\\n\"\n",
    "                        else:\n",
    "                            news_content += part.text.strip() + \"\\n\"\n",
    "                    elif part.name == \"h2\":\n",
    "                        news_content +=\"<subquestion>\" + part.text.strip()+ \"</subquestion>\" + \"\\n\"\n",
    "                    else:\n",
    "                        news_content += part.text.strip() + \"\\n\"\n",
    "                    prev_part = part\n",
    "                # pprint(news_content)\n",
    "                item['content'] = news_content\n",
    "                # pprint(item)\n",
    "                refs = content.find_all('a')\n",
    "                item['refs'] = [ref.text.strip() for ref in refs]\n",
    "\n",
    "            return item \n",
    "        else:\n",
    "            print(\"Error when get page content\")\n",
    "            with open(\"./error.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(url + \"\\n\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(\"Error when get page content\")\n",
    "        with open(\"./error.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(url + \"\\n\")\n",
    "        return {}\n",
    "\n",
    "def get_content_from_major(major):\n",
    "    # page = 0\n",
    "    MAJOR = major\n",
    "    url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/{major}?page=\"\n",
    "    for page in range(1,120):\n",
    "        url = url + str(page)\n",
    "        print(url)\n",
    "        page_links = requests.get(url)\n",
    "        soup = BeautifulSoup(page_links.text, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a', class_='title-link')\n",
    "        \n",
    "        if len(links) == 0:\n",
    "            break\n",
    "            # break\n",
    "        for link in links:\n",
    "            # print(link['href'])\n",
    "            time.sleep(1.5)\n",
    "            page_content = get_page_content(link['href'])\n",
    "            if page_content != {}:\n",
    "                page_content['domain'] = major\n",
    "                page_content['url'] = link['href']\n",
    "                page_content['crawled_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "                with open(f\"./data_qa_new_{major}.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(page_content, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/{major}?page=\"\n",
    "        \n",
    "        with open('./processed_major.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(major + \" \" + str(page) + \"\\n\")\n",
    "            \n",
    "        time.sleep(2)\n",
    "\n",
    "def get_content_from_category(category):\n",
    "    url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/chu-de/{category}?page=\"\n",
    "    for page in range(1,100):\n",
    "        url = url + str(page)\n",
    "        print(url)\n",
    "        page_links = requests.get(url)\n",
    "        soup = BeautifulSoup(page_links.text, 'html.parser')\n",
    "        links = soup.find_all('a', class_='title-link')\n",
    "        \n",
    "        if len(links) == 0:\n",
    "            break\n",
    "        \n",
    "        for link in links:\n",
    "            # print(link['href'])\n",
    "            time.sleep(1.5)\n",
    "            page_content = get_page_content(link['href'])\n",
    "            if page_content != {}:\n",
    "                page_content['domain'] = category\n",
    "                page_content['url'] = link['href']\n",
    "                page_content['crawled_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "                with open(\"./data_qa_new.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(page_content, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/chu-de/{category}?page=\"\n",
    "        \n",
    "        with open('./processed_category.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(category + \" \" + str(page) + \"\\n\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "            \n",
    "\n",
    "# get_content_from_major(\"tien-te-ngan-hang\")\n",
    "\n",
    "majors = [ \n",
    "\t'trach-nhiem-hinh-su', 'xay-dung-do-thi', 'ke-toan-kiem-toan', 'thue-phi-le-phi',\n",
    "          'dau-tu', 'dich-vu-phap-ly', 'tai-nguyen-moi-truong', 'cong-nghe-thong-tin', 'giao-duc',\n",
    "          'bo-may-hanh-chinh', 'linh-vuc-khac']\n",
    "\n",
    "categories = ['kinh-doanh-van-tai', 'nghia-vu-quan-su', 'thua-ke', 'thue-gia-tri-gia-tang',\n",
    "              'bien-so-xe', 'thu-tuc-ly-hon', 'che-do-thai-san', 'so-bao-hiem-xa-hoi', 'the-bao-hiem-y-te',\n",
    "              'tro-cap-thoi-viec', 'muc-luong-toi-thieu', 'giam-tru-gia-canh', 'thoi-han-su-dung-dat',\n",
    "              'giay-khai-sinh', 'vung-nuoc-cang-bien',  'ngach-cong-chuc']\n",
    "\n",
    "\n",
    "for major in majors:\n",
    "    get_content_from_major(major)\n",
    "\n",
    "# for category in categories:\n",
    "#     get_content_from_category(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be136828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c84d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
