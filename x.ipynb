{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932089",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-24T04:19:46.612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=1\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=2\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=3\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=4\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=5\n",
      "https://thuvienphapluat.vn/hoi-dap-phap-luat/trach-nhiem-hinh-su?page=6\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "MAJOR = \"\"\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"Get html from url\"\"\"\n",
    "    try:\n",
    "        page_content = requests.get(url)\n",
    "\n",
    "        if page_content.status_code == 200:\n",
    "            soup = BeautifulSoup(page_content.text, 'html.parser')\n",
    "\n",
    "            item = {}\n",
    "            #get date\n",
    "            date = soup.find('span', class_='news-time')\n",
    "            if date:\n",
    "                item['published_date'] = date.text.strip()\n",
    "            # Get title\n",
    "            title = soup.find('h1', class_='title')\n",
    "            if title:\n",
    "                item['title'] = title.text.strip()\n",
    "            # Get content\n",
    "            content = soup.find('section', id=\"news-content\")\n",
    "            if content:\n",
    "                #get question\n",
    "                question = content.find('strong', class_=\"sapo\")\n",
    "                if question:\n",
    "                    item['question'] = question.text.strip()\n",
    "                #remove question\n",
    "                question.extract()\n",
    "                #remove div with id= \"accordionMucLuc\" from content\n",
    "                accordion = content.find('div', id=\"accordionMucLuc\")\n",
    "                if accordion:\n",
    "                    accordion.extract()\n",
    "                #get content\n",
    "                # item['content'] = content.text.strip()\n",
    "                parts = content.find_all(['p','h2', 'blockquote'])\n",
    "                # pprint(parts)\n",
    "                news_content = \"\"\n",
    "                prev_part = None\n",
    "                for part in parts[:-1]:\n",
    "                    if part.name == \"p\":\n",
    "                        if prev_part and prev_part.name == \"blockquote\":\n",
    "                            news_content += \"</ref>\\n\" + part.text.strip() + \"\\n\"\n",
    "                        else:\n",
    "                            news_content += part.text.strip() + \"\\n\"\n",
    "                    elif part.name == \"blockquote\":\n",
    "                        if prev_part and prev_part.name != \"blockquote\":\n",
    "                            news_content += \"<ref>\" + part.text.strip() + \"\\n\"\n",
    "                        else:\n",
    "                            news_content += part.text.strip() + \"\\n\"\n",
    "                    elif part.name == \"h2\":\n",
    "                        news_content +=\"<subquestion>\" + part.text.strip()+ \"</subquestion>\" + \"\\n\"\n",
    "                    else:\n",
    "                        news_content += part.text.strip() + \"\\n\"\n",
    "                    prev_part = part\n",
    "                # pprint(news_content)\n",
    "                item['content'] = news_content\n",
    "                # pprint(item)\n",
    "                refs = content.find_all('a')\n",
    "                item['refs'] = [ref.text.strip() for ref in refs]\n",
    "\n",
    "            return item \n",
    "        else:\n",
    "            print(\"Error when get page content\")\n",
    "            with open(\"./error.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(url + \"\\n\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(\"Error when get page content\")\n",
    "        with open(\"./error.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(url + \"\\n\")\n",
    "        return {}\n",
    "\n",
    "def get_content_from_major(major):\n",
    "    # page = 0\n",
    "    MAJOR = major\n",
    "    url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/{major}?page=\"\n",
    "    for page in range(1,120):\n",
    "        url = url + str(page)\n",
    "        print(url)\n",
    "        page_links = requests.get(url)\n",
    "        soup = BeautifulSoup(page_links.text, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a', class_='title-link')\n",
    "        \n",
    "        if len(links) == 0:\n",
    "            break\n",
    "            # break\n",
    "        for link in links:\n",
    "            # print(link['href'])\n",
    "            time.sleep(1.5)\n",
    "            page_content = get_page_content(link['href'])\n",
    "            if page_content != {}:\n",
    "                page_content['domain'] = major\n",
    "                page_content['url'] = link['href']\n",
    "                page_content['crawled_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "                with open(f\"./data_qa_new_{major}.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(page_content, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/{major}?page=\"\n",
    "        \n",
    "        with open('./processed_major.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(major + \" \" + str(page) + \"\\n\")\n",
    "            \n",
    "        time.sleep(2)\n",
    "\n",
    "def get_content_from_category(category):\n",
    "    url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/chu-de/{category}?page=\"\n",
    "    for page in range(1,100):\n",
    "        url = url + str(page)\n",
    "        print(url)\n",
    "        page_links = requests.get(url)\n",
    "        soup = BeautifulSoup(page_links.text, 'html.parser')\n",
    "        links = soup.find_all('a', class_='title-link')\n",
    "        \n",
    "        if len(links) == 0:\n",
    "            break\n",
    "        \n",
    "        for link in links:\n",
    "            # print(link['href'])\n",
    "            time.sleep(1.5)\n",
    "            page_content = get_page_content(link['href'])\n",
    "            if page_content != {}:\n",
    "                page_content['domain'] = category\n",
    "                page_content['url'] = link['href']\n",
    "                page_content['crawled_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "                with open(\"./data_qa_new.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(page_content, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/chu-de/{category}?page=\"\n",
    "        \n",
    "        with open('./processed_category.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(category + \" \" + str(page) + \"\\n\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "            \n",
    "\n",
    "# get_content_from_major(\"tien-te-ngan-hang\")\n",
    "\n",
    "majors = [ \n",
    "\t'trach-nhiem-hinh-su', 'xay-dung-do-thi', 'ke-toan-kiem-toan', 'thue-phi-le-phi',\n",
    "          'dau-tu', 'dich-vu-phap-ly', 'tai-nguyen-moi-truong', 'cong-nghe-thong-tin', 'giao-duc',\n",
    "          'bo-may-hanh-chinh', 'linh-vuc-khac']\n",
    "\n",
    "categories = ['kinh-doanh-van-tai', 'nghia-vu-quan-su', 'thua-ke', 'thue-gia-tri-gia-tang',\n",
    "              'bien-so-xe', 'thu-tuc-ly-hon', 'che-do-thai-san', 'so-bao-hiem-xa-hoi', 'the-bao-hiem-y-te',\n",
    "              'tro-cap-thoi-viec', 'muc-luong-toi-thieu', 'giam-tru-gia-canh', 'thoi-han-su-dung-dat',\n",
    "              'giay-khai-sinh', 'vung-nuoc-cang-bien',  'ngach-cong-chuc']\n",
    "\n",
    "\n",
    "for major in majors:\n",
    "    get_content_from_major(major)\n",
    "\n",
    "# for category in categories:\n",
    "#     get_content_from_category(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22da467a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T04:38:25.808938Z",
     "start_time": "2024-07-24T04:38:25.301630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n",
      "https://cafe.naver.com/tradeforwarding1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "MAJOR = \"\"\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"Get html from url\"\"\"\n",
    "    try:\n",
    "        page_content = requests.get(url)\n",
    "\n",
    "        if page_content.status_code == 200:\n",
    "            soup = BeautifulSoup(page_content.text, 'html.parser')\n",
    "\n",
    "            item = {}\n",
    "            #get date\n",
    "            date = soup.find('span', class_='news-time')\n",
    "            if date:\n",
    "                item['published_date'] = date.text.strip()\n",
    "            # Get title\n",
    "            title = soup.find('h1', class_='title')\n",
    "            if title:\n",
    "                item['title'] = title.text.strip()\n",
    "            # Get content\n",
    "            content = soup.find('section', id=\"news-content\")\n",
    "            if content:\n",
    "                #get question\n",
    "                question = content.find('strong', class_=\"sapo\")\n",
    "                if question:\n",
    "                    item['question'] = question.text.strip()\n",
    "                #remove question\n",
    "                question.extract()\n",
    "                #remove div with id= \"accordionMucLuc\" from content\n",
    "                accordion = content.find('div', id=\"accordionMucLuc\")\n",
    "                if accordion:\n",
    "                    accordion.extract()\n",
    "                #get content\n",
    "                # item['content'] = content.text.strip()\n",
    "                parts = content.find_all(['p','h2', 'blockquote'])\n",
    "                # pprint(parts)\n",
    "                news_content = \"\"\n",
    "                prev_part = None\n",
    "                for part in parts[:-1]:\n",
    "                    if part.name == \"p\":\n",
    "                        if prev_part and prev_part.name == \"blockquote\":\n",
    "                            news_content += \"</ref>\\n\" + part.text.strip() + \"\\n\"\n",
    "                        else:\n",
    "                            news_content += part.text.strip() + \"\\n\"\n",
    "                    elif part.name == \"blockquote\":\n",
    "                        if prev_part and prev_part.name != \"blockquote\":\n",
    "                            news_content += \"<ref>\" + part.text.strip() + \"\\n\"\n",
    "                        else:\n",
    "                            news_content += part.text.strip() + \"\\n\"\n",
    "                    elif part.name == \"h2\":\n",
    "                        news_content +=\"<subquestion>\" + part.text.strip()+ \"</subquestion>\" + \"\\n\"\n",
    "                    else:\n",
    "                        news_content += part.text.strip() + \"\\n\"\n",
    "                    prev_part = part\n",
    "                # pprint(news_content)\n",
    "                item['content'] = news_content\n",
    "                # pprint(item)\n",
    "                refs = content.find_all('a')\n",
    "                item['refs'] = [ref.text.strip() for ref in refs]\n",
    "\n",
    "            return item \n",
    "        else:\n",
    "            print(\"Error when get page content\")\n",
    "            with open(\"./error.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(url + \"\\n\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(\"Error when get page content\")\n",
    "        with open(\"./error.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(url + \"\\n\")\n",
    "        return {}\n",
    "\n",
    "def get_content_from_major(major):\n",
    "    # page = 0\n",
    "    MAJOR = major\n",
    "    url = f\"https://cafe.naver.com/tradeforwarding\"\n",
    "    for page in range(1,120):\n",
    "        url = url + str(page)\n",
    "        print(url)\n",
    "        page_links = requests.get(url)\n",
    "        soup = BeautifulSoup(page_links.text, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('a', class_='title-link')\n",
    "        \n",
    "        if len(links) == 0:\n",
    "            break\n",
    "            # break\n",
    "        for link in links:\n",
    "            # print(link['href'])\n",
    "            time.sleep(1.5)\n",
    "            page_content = get_page_content(link['href'])\n",
    "            if page_content != {}:\n",
    "                page_content['domain'] = major\n",
    "                page_content['url'] = link['href']\n",
    "                page_content['crawled_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "                with open(f\"./data_qa_new_{major}.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(page_content, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        url = f\"https://cafe.naver.com/tradeforwarding\"\n",
    "        \n",
    "        with open('./processed_major.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(major + \" \" + str(page) + \"\\n\")\n",
    "            \n",
    "        time.sleep(2)\n",
    "\n",
    "def get_content_from_category(category):\n",
    "    url = f\"https://cafe.naver.com/tradeforwarding\"\n",
    "    for page in range(1,100):\n",
    "        url = url + str(page)\n",
    "        print(url)\n",
    "        page_links = requests.get(url)\n",
    "        soup = BeautifulSoup(page_links.text, 'html.parser')\n",
    "        links = soup.find_all('a', class_='title-link')\n",
    "        \n",
    "        if len(links) == 0:\n",
    "            break\n",
    "        \n",
    "        for link in links:\n",
    "            # print(link['href'])\n",
    "            time.sleep(1.5)\n",
    "            page_content = get_page_content(link['href'])\n",
    "            if page_content != {}:\n",
    "                page_content['domain'] = category\n",
    "                page_content['url'] = link['href']\n",
    "                page_content['crawled_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "                with open(\"./data_qa_new.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(page_content, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        url = f\"https://thuvienphapluat.vn/hoi-dap-phap-luat/chu-de/{category}?page=\"\n",
    "        \n",
    "        with open('./processed_category.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(category + \" \" + str(page) + \"\\n\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "            \n",
    "\n",
    "# get_content_from_major(\"tien-te-ngan-hang\")\n",
    "\n",
    "majors = [#'tien-te-ngan-hang', 'quyen-dan-su','chung-khoan', 'so-huu-tri-tue', 'tai-chinh-nha-nuoc',\n",
    "          #'thu-tuc-to-tung', 'the-thao-y-te', 'giao-thong-van-tai', 'xuat-nhap-khau', 'doanh-nghiep',\n",
    "          #'lao-dong-tien-luong', 'bat-dong-san', 'vi-pham-hanh-chinh', 'bao-hiem', 'van-hoa-xa-hoi',\n",
    "          #'thuong-mai', \n",
    "\t'trach-nhiem-hinh-su', 'xay-dung-do-thi', 'ke-toan-kiem-toan', 'thue-phi-le-phi',\n",
    "          'dau-tu', 'dich-vu-phap-ly', 'tai-nguyen-moi-truong', 'cong-nghe-thong-tin', 'giao-duc',\n",
    "          'bo-may-hanh-chinh', 'linh-vuc-khac']\n",
    "\n",
    "\n",
    "\n",
    "for major in majors:\n",
    "    get_content_from_major(major)\n",
    "\n",
    "# for category in categories:\n",
    "#     get_content_from_category(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "901d8d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:08:07.323141Z",
     "start_time": "2024-07-24T05:08:00.608564Z"
    }
   },
   "outputs": [
    {
     "ename": "NoSuchFrameException",
     "evalue": "Message: cafe_main\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\switch_to.py:88\u001b[0m, in \u001b[0;36mSwitchTo.frame\u001b[1;34m(self, frame_reference)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     frame_reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_reference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:739\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    737\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"cafe_main\"]\"}\n  (Session info: chrome=126.0.6478.128); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6117AEEB2+31554]\n\t(No symbol) [0x00007FF611727EE9]\n\t(No symbol) [0x00007FF6115E872A]\n\t(No symbol) [0x00007FF611638434]\n\t(No symbol) [0x00007FF61163853C]\n\t(No symbol) [0x00007FF61167F6A7]\n\t(No symbol) [0x00007FF61165D06F]\n\t(No symbol) [0x00007FF61167C977]\n\t(No symbol) [0x00007FF61165CDD3]\n\t(No symbol) [0x00007FF61162A33B]\n\t(No symbol) [0x00007FF61162AED1]\n\tGetHandleVerifier [0x00007FF611AB8B2D+3217341]\n\tGetHandleVerifier [0x00007FF611B05AF3+3532675]\n\tGetHandleVerifier [0x00007FF611AFB0F0+3489152]\n\tGetHandleVerifier [0x00007FF61185E786+750614]\n\t(No symbol) [0x00007FF61173376F]\n\t(No symbol) [0x00007FF61172EB24]\n\t(No symbol) [0x00007FF61172ECB2]\n\t(No symbol) [0x00007FF61171E17F]\n\tBaseThreadInitThunk [0x00007FFEFE7F7374+20]\n\tRtlUserThreadStart [0x00007FFEFF0FCC91+33]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\switch_to.py:91\u001b[0m, in \u001b[0;36mSwitchTo.frame\u001b[1;34m(self, frame_reference)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     frame_reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_reference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:739\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    737\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[name=\"cafe_main\"]\"}\n  (Session info: chrome=126.0.6478.128); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6117AEEB2+31554]\n\t(No symbol) [0x00007FF611727EE9]\n\t(No symbol) [0x00007FF6115E872A]\n\t(No symbol) [0x00007FF611638434]\n\t(No symbol) [0x00007FF61163853C]\n\t(No symbol) [0x00007FF61167F6A7]\n\t(No symbol) [0x00007FF61165D06F]\n\t(No symbol) [0x00007FF61167C977]\n\t(No symbol) [0x00007FF61165CDD3]\n\t(No symbol) [0x00007FF61162A33B]\n\t(No symbol) [0x00007FF61162AED1]\n\tGetHandleVerifier [0x00007FF611AB8B2D+3217341]\n\tGetHandleVerifier [0x00007FF611B05AF3+3532675]\n\tGetHandleVerifier [0x00007FF611AFB0F0+3489152]\n\tGetHandleVerifier [0x00007FF61185E786+750614]\n\t(No symbol) [0x00007FF61173376F]\n\t(No symbol) [0x00007FF61172EB24]\n\t(No symbol) [0x00007FF61172ECB2]\n\t(No symbol) [0x00007FF61171E17F]\n\tBaseThreadInitThunk [0x00007FFEFE7F7374+20]\n\tRtlUserThreadStart [0x00007FFEFF0FCC91+33]\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNoSuchFrameException\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m page \u001b[38;5;241m<\u001b[39m num_page:\n\u001b[0;32m     58\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cafe.naver.com/ArticleList.nhn?search.clubid=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m                \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(clubid)\n\u001b[0;32m     60\u001b[0m                \u001b[38;5;66;03m#+\"&search.menuid=\"+str(menuid)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m                \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&search.boardtype=L&search.totalCount=151&search.cafeId=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(clubid)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&search.page=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(page \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitch_to\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcafe_main\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 페이지 로딩 시간\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     driver\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\switch_to.py:93\u001b[0m, in \u001b[0;36mSwitchTo.frame\u001b[1;34m(self, frame_reference)\u001b[0m\n\u001b[0;32m     91\u001b[0m             frame_reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mNAME, frame_reference)\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 93\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m NoSuchFrameException(frame_reference) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mSWITCH_TO_FRAME, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: frame_reference})\n",
      "\u001b[1;31mNoSuchFrameException\u001b[0m: Message: cafe_main\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame([],columns=[\"title\",\"Post number\",\"Date\",\"ID\",\"Nickname\",\"Image Link\",\"Image save path\",\"Commenter ID\",\"Commenter Nickname\",\n",
    "        \"Comment\",\"Comment Date\",\"Reply ID\",\"Reply Nickname\",\"Reply\",\"Reply Date\",],)\n",
    "\n",
    "# driver = webdriver.Chrome()\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Naver login url / your id / your passward\n",
    "url='https://nid.naver.com/nidlogin.login'\n",
    "id_ = 'doanvo01'\n",
    "pw = 'Td201096@@'\n",
    "    \n",
    "driver.get(url)\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "# Naver login 네이버 로그인\n",
    "driver.execute_script(\"document.getElementsByName('id')[0].value=\\'\"+ id_ + \"\\'\")\n",
    "driver.execute_script(\"document.getElementsByName('pw')[0].value=\\'\"+ pw + \"\\'\")\n",
    "driver.find_element(by=By.XPATH,value='//*[@id=\"log.login\"]').click()\n",
    "time.sleep(1)\n",
    "    \n",
    "# wanted naver cafe url\n",
    "baseurl='https://cafe.naver.com/(name of naver cafe)/'\n",
    "clubid = '(write club id number)' # what is your naver cafe's clubid? / 네이버 카페 클럽 아이디 입력\n",
    "#menuid = '(write menu id number)' # what is your naver cafe's menuid? / 네이버 카페 클럽 게시판 입력(필요시)\n",
    "\n",
    "# login time you should login within 2 sec\n",
    "time.sleep(1)\n",
    "\n",
    "# ASSUME LOGIN  SUCCESS\n",
    "num_page = 2  # how many pages do you want? / 총 페이지 수\n",
    "\n",
    "##########################################################################\n",
    "# do not touch\n",
    "page = 0\n",
    "index = 0\n",
    "\n",
    "while page < num_page:\n",
    "    driver.get(\"https://cafe.naver.com/ArticleList.nhn?search.clubid=\"\n",
    "               +str(clubid)\n",
    "               #+\"&search.menuid=\"+str(menuid)\n",
    "               +\"&search.boardtype=L&search.totalCount=151&search.cafeId=\"+str(clubid)+\"&search.page=\"+ str(page + 1))\n",
    "    driver.switch_to.frame(\"cafe_main\")\n",
    "    \n",
    "    time.sleep(1)  # 페이지 로딩 시간\n",
    "    driver.implicitly_wait(1)\n",
    "   \n",
    "    # BeautifulSoup으로 HTML을 파싱\n",
    "    driver_page_source = driver.page_source\n",
    "    soup = bs(driver_page_source, 'html.parser')\n",
    "\n",
    "    # 해당 class를 가진 모든 게시글 링크들을 찾음\n",
    "    article = soup.find_all(class_=\"inner_list\")\n",
    "\n",
    "    links = []\n",
    "    post_num_list = []\n",
    "    find_one = 0\n",
    "    for idx, link in enumerate(article):\n",
    "        idid = link.find(class_='article')['href'].split('articleid=')[-1]\n",
    "        if idid[-1] == 'e':\n",
    "            if find_one == 0:\n",
    "                find_idx = idx\n",
    "                find_one += 10\n",
    "            idid = idid.split('&')[0]\n",
    "        post_num_list.append(int(idid))\n",
    "        links.append(baseurl + idid)\n",
    "\n",
    "    wow_gongi = pd.read_html(driver_page_source)[0].iloc[[_ for _ in range(0,len(pd.read_html(driver_page_source)[0]),2)],[1,2,3,4]]\n",
    "    wow_gongi = wow_gongi.reset_index(drop=True)\n",
    "    wow_gongi.iloc[:,1] = wow_gongi.iloc[:,1].str.split('w').str[0]\n",
    "    \n",
    "    text_column = wow_gongi.columns\n",
    "    \n",
    "    wow = pd.read_html(driver_page_source)[find_idx+1].iloc[[_ for _ in range(0,len(pd.read_html(driver_page_source)[find_idx+1]),2)],[1,2,3,4]]\n",
    "    wow = wow.reset_index(drop=True)\n",
    "    wow.iloc[:,1] = wow.iloc[:,1].str.split('w').str[0]\n",
    "    wow.columns = text_column\n",
    "    \n",
    "    wow = pd.concat([wow_gongi, wow], axis=0)\n",
    "    wow = pd.concat([wow.reset_index(drop=True), pd.DataFrame({'번호': post_num_list})], axis=1)\n",
    "    wow = pd.concat([wow, pd.DataFrame({'링크': links})], axis=1)\n",
    "    \n",
    "    idx_wow = 0\n",
    "    while idx_wow < len(wow):\n",
    "        print(page + 1, \"번 페이지\", idx_wow + 1, \"번째 게시물\")\n",
    "        post_num = wow.iloc[idx_wow,4]\n",
    "        print('글 번호:',post_num)          \n",
    "        driver.get(wow.iloc[idx_wow,5])\n",
    "        driver.switch_to.frame(\"cafe_main\")\n",
    "        time.sleep(1)\n",
    "        driver.implicitly_wait(1)\n",
    "        \n",
    "        # BeautifulSoup으로 HTML을 파싱\n",
    "        another_soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "        # 해당 class를 가진 모든 게시글 링크들을 찾음\n",
    "        another_article = another_soup.find_all(class_=\"inner_list\")     \n",
    "        \n",
    "        # Title\n",
    "        title = driver.find_element(By.CLASS_NAME, \"title_text\").text\n",
    "\n",
    "        # Date\n",
    "        date = driver.find_element(By.CLASS_NAME, \"date\").text\n",
    "\n",
    "        # Nickname\n",
    "        nickname = driver.find_element(By.CLASS_NAME, \"nickname\").text\n",
    "\n",
    "        # Writer ID\n",
    "        writer_info = driver.find_element(By.CLASS_NAME, \"thumb\").get_attribute(\"href\")\n",
    "        writer_id = \"\"\n",
    "        if \"members/\" in writer_info:\n",
    "            writer_id = writer_info[writer_info.index(\"members/\") + 8 :]\n",
    "\n",
    "        # Image link\n",
    "        image_list = driver.find_elements(By.CLASS_NAME, \"se-image-resource\")\n",
    "\n",
    "        image = \"\"\"\"\"\"\n",
    "        image_dir = \"\"\"\"\"\"\n",
    "        count = 0\n",
    "        \n",
    "        if not os.path.isdir(\"save_images\"):\n",
    "            os.mkdir(\"save_images\")\n",
    "        \n",
    "        for im in image_list:\n",
    "            url = im.get_attribute(\"src\")\n",
    "            lcs_add = \"save_images/img_\" + str(index) + \"_\" + str(count + 1) + \".jpg\"\n",
    "            urlretrieve(url, lcs_add)  # download image into directory\n",
    "\n",
    "            image_dir += \"save_images/img_\" + str(index) + \"_\" + str(count + 1) + \".jpg\"\n",
    "            image_dir += \"\\n\"\n",
    "\n",
    "            image += im.get_attribute(\"src\")\n",
    "            image += \"\\n\"\n",
    "            count += 1\n",
    "\n",
    "        # Nickname of commenter & Comment\n",
    "        comtemp_list = another_soup.find_all('span', {'class':'text_comment'})\n",
    "        commenter_1_list = []\n",
    "        comment_1_list = []\n",
    "        comment_time_1_list = []\n",
    "        commenter_id_1_list = []\n",
    "        for idx in range(len(comtemp_list)):\n",
    "            another_soup_find_all_div_class_comment_area = another_soup.find_all('div', {'class':'comment_area'})[idx]\n",
    "\n",
    "            if another_soup_find_all_div_class_comment_area.text.strip() == '삭제된 댓글입니다.':\n",
    "                commenter_1_list.append('Deleted')\n",
    "                comment_1_list.append('Deleted')\n",
    "                comment_time_1_list.append('Deleted')\n",
    "                commenter_id_1_list.append('Deleted')\n",
    "                continue\n",
    "                    \n",
    "            commenter = another_soup_find_all_div_class_comment_area.find_all('a', {'aria-expanded':'false'})[0].text.strip()\n",
    "            comment = another_soup_find_all_div_class_comment_area.find_all('span', {'class':'text_comment'})[0].text\n",
    "            comment_time = another_soup_find_all_div_class_comment_area.find_all('span', {'class':'comment_info_date'})[0].text   \n",
    "            \n",
    "            commenter_1_list.append(commenter)\n",
    "            comment_1_list.append(comment)\n",
    "            comment_time_1_list.append(comment_time)\n",
    "            \n",
    "            # Commenter ID\n",
    "            comment_id = another_soup_find_all_div_class_comment_area.find_all('a', {'class':'comment_thumb'})[0]['href'].split('/')[-1]\n",
    "            commenter_id_1_list.append(comment_id)\n",
    "                \n",
    "        if len(comtemp_list) == 0:\n",
    "            commenter_1_list.append(\"NO COMMENT\")\n",
    "            comment_1_list.append(\"NO COMMENT\")\n",
    "            comment_time_1_list.append(\"NO COMMENT\")\n",
    "            commenter_id_1_list.append(\"NO COMMENT\")\n",
    "\n",
    "        if idx_wow < len(wow) - 1:\n",
    "            idx_wow += 1\n",
    "        else:\n",
    "            print(\"ALL posts comsumed\\nGO TO NEXT PAGE\")\n",
    "            page += 1\n",
    "            idx_wow = 0            \n",
    "            \n",
    "            driver.get(\"https://cafe.naver.com/ArticleList.nhn?search.clubid=\"\n",
    "               +str(clubid)\n",
    "               #+\"&search.menuid=\"+str(menuid)\n",
    "               +\"&search.boardtype=L&search.totalCount=151&search.cafeId=\"\n",
    "               +str(clubid)\n",
    "               +\"&search.page=\"\n",
    "               + str(page + 1))\n",
    "            driver.switch_to.frame(\"cafe_main\")\n",
    "    \n",
    "            time.sleep(1)  # 페이지 로딩 시간\n",
    "            driver.implicitly_wait(1)\n",
    "            print(page + 1, \" 번 페이지\", idx_wow + 1, \"번째 게시물\")\n",
    "            #############################################\n",
    "            \n",
    "            # BeautifulSoup으로 HTML을 파싱\n",
    "            driver_page_source = driver.page_source\n",
    "            soup = bs(driver_page_source, 'html.parser')\n",
    "\n",
    "            # 해당 class를 가진 모든 게시글 링크들을 찾음\n",
    "            article = soup.find_all(class_=\"inner_list\")\n",
    "\n",
    "            links = []\n",
    "            post_num_list = []\n",
    "            find_one = 0\n",
    "            for idx, link in enumerate(article):\n",
    "                idid = link.find(class_='article')['href'].split('articleid=')[-1]\n",
    "                if idid[-1] == 'e':\n",
    "                    if find_one == 0:\n",
    "                        find_idx = idx\n",
    "                        find_one += 10\n",
    "                    idid = idid.split('&')[0]\n",
    "                post_num_list.append(int(idid))\n",
    "                links.append(baseurl + idid)\n",
    "            \n",
    "            wow = pd.read_html(driver_page_source)[1].iloc[[_ for _ in range(0,len(pd.read_html(driver_page_source)[1]),2)],[1,2,3,4]]\n",
    "            wow = wow.reset_index(drop=True)\n",
    "            wow.iloc[:,1] = wow.iloc[:,1].str.split('w').str[0]\n",
    "            wow.columns = text_column\n",
    "            wow = pd.concat([wow.reset_index(drop=True), pd.DataFrame({'번호': post_num_list})], axis=1)\n",
    "            wow = pd.concat([wow, pd.DataFrame({'링크': links})], axis=1)\n",
    "\n",
    "        # Go to main page to track reply page\n",
    "        driver.get(\"https://cafe.naver.com/ArticleList.nhn?search.clubid=\"\n",
    "                   +str(clubid)\n",
    "                   #+\"&search.menuid=\"+str(menuid)\n",
    "                   +\"&search.boardtype=L&search.totalCount=151&search.cafeId=\"\n",
    "                   +str(clubid)\n",
    "                   +\"&search.page=\"\n",
    "                   + str(page + 1))\n",
    "        driver.switch_to.frame(\"cafe_main\")\n",
    "        time.sleep(1)\n",
    "        driver.implicitly_wait(1)\n",
    "\n",
    "        # Reply Test\n",
    "        # if not a reply, visit again\n",
    "        driver.get(wow.iloc[idx_wow,5])\n",
    "        driver.switch_to.frame(\"cafe_main\")\n",
    "        driver.implicitly_wait(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # reply title\n",
    "        reply_title = driver.find_element(By.CLASS_NAME, \"title_text\").text\n",
    "\n",
    "        # Reply if title = reply_title\n",
    "        if title == reply_title:\n",
    "            print(\"It's reply\")\n",
    "            reply_date = driver.find_element(By.CLASS_NAME, \"date\").text\n",
    "            reply_nickname = driver.find_element(By.CLASS_NAME, \"nickname\").text\n",
    "            reply_text = \"\"\"\"\"\"  # se-fs- se-ff-\n",
    "            for info in driver.find_elements(By.CSS_SELECTOR, \".se-fs-.se-ff-\"):\n",
    "                reply_text += info.text\n",
    "                reply_text += \" \"\n",
    "\n",
    "            # Reply ID\n",
    "            reply_info = driver.find_element(By.CLASS_NAME, \"thumb\").get_attribute(\"href\")\n",
    "            reply_id = \"\"\n",
    "            if \"members/\" in reply_info:\n",
    "                reply_id = reply_info[reply_info.index(\"members/\") + 8 :]\n",
    "\n",
    "            # To the next page\n",
    "            idx_wow += 1\n",
    "        else:\n",
    "            print(\"It's NOT reply. Stay on the same page\")\n",
    "            reply_date = \"No reply\"\n",
    "            reply_nickname = \"No reply\"\n",
    "            reply_text = \"No reply\"\n",
    "            reply_id = \"No reply\"\n",
    "\n",
    "        # 16 columns\n",
    "        df.loc[index] = [title,post_num,date, writer_id, nickname, image, image_dir, commenter_id_1_list, commenter_1_list, comment_1_list, comment_time_1_list, reply_id, reply_nickname, reply_text, reply_date,        ]\n",
    "        df.to_csv(r\"test.csv\",encoding=\"utf-8-sig\",index=False,)\n",
    "        \n",
    "        # Go to main page\n",
    "        driver.get(\"https://cafe.naver.com/ArticleList.nhn?search.clubid=\"\n",
    "                   +str(clubid)\n",
    "                   #+\"&search.menuid=\"+str(menuid)\n",
    "                   +\"&search.boardtype=L&search.totalCount=151&search.cafeId=\"+str(clubid)+\"&search.page=\"+ str(page + 1))\n",
    "        driver.implicitly_wait(1)\n",
    "        driver.switch_to.frame(\"cafe_main\")\n",
    "        time.sleep(1)\n",
    "        index += 1\n",
    "        if page >= num_page:\n",
    "            break\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be136828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c84d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
